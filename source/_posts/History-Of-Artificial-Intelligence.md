---
title: 人工智能史
toc: true
categories:
  - AI
tags: [课程]
date: 2024-10-13 15:06:54
updated: 2026-01-12 00:14:00
---

本文是 Björn Schuller 教授的人工智能课程的第一部分。

## 发展

**人工智能（Artificial Intelligence，AI）** 是以机器为载体所展示的人类智能，旨在模仿和超越人类的认知与能力。

| 阶段       | 时间    | 标志或原因                                         |
| ---------- | ------- | -------------------------------------------------- |
| 第一次高潮 | 1956 年 | 达特茅斯会议；机器定理证明，棋类博弈               |
| 第一次寒冬 | 1960s   | 莱特希尔报告；只能解决玩具问题；组合爆炸，机器翻译 |
| 第二次高潮 | 1970s   | 专家系统，知识工程                                 |
| 第二次寒冬 | 1980s   | 没有达到人们的期望；第五代计算机研制失败           |
| 稳步发展期 | 1990s   | 深度学习取得成功；围棋博弈，蛋白质结构预测         |
| 全面爆发期 | 21 世纪 | 深度学习的全面突破                                 |

<!-- more -->

![AI History 1](./AI-History-1.png)

![AI History 2](./AI-History-2.png)

![AI History 3](./AI-History-3.png)

## 流派

针对人工智能的本质和实现方法，有三大主要流派：**符号主义（Symbolism）、连接主义（Connectionism）和行为主义（Behaviorism）**。

| 流派     | 主要观点                           | 代表方法与特点                         | 应用实例               |
| -------- | ---------------------------------- | -------------------------------------- | ---------------------- |
| 符号主义 | 智能是通过符号和逻辑的操作实现的   | 专家系统，知识库，强调推理和知识过程   | 医疗诊断，机器定理证明 |
| 连接主义 | 智能是通过神经元的连接和交互产生的 | 人工神经网络，深度学习，强调算力与数据 | 图像识别，自然语言处理 |
| 行为主义 | 智能是通过与环境的交互和适应形成的 | 强化学习，试错和奖惩，强调自我优化     | 游戏智能体，机器人控制 |

## AI/ML/DL

**机器学习（Machine Learning，ML）** 是人工智能的子集，是人工智能的一种实现方法：让机器通过学习大量数据（Data）来获得完成特定任务的能力。

**深度学习（Deep Learning，DL）** 是机器学习的子集，是机器学习的一种实现方法：通过构建多层的神经网络（此之谓“深”）来完成机器学习。

![AI/ML/DL](./AI-ML-DL.png)

此外，生成式人工智能（GenAI）是人工智能的子集，且与机器学习和深度学习都有交集；大数据（Big Data）不是人工智能的子集，但是与人工智能有交集。

## 机器学习导言

机器学习旨在发现规则以执行一个数据处理任务。一个机器学习系统是**被训练出来的**，而非被显式地编程出来的。并且与优化问题和常规统计分析不同，我们想要学习**能够泛化到新数据**的规则。

### 要素

要想实现机器学习，通常需要：

- **输入数据点**：例如对于语音识别模型而言，输入数据点可以是一些人类的语音片段；
- **预期输出的示例**：例如对于语音识别模型而言，预期输出的示例可以是对应的文本转录；
- **一个用于评价模型表现的算法**：衡量模型输出和预期输出的差异，并对模型给予反馈信号以便调整和学习。

### 关键概念

- **特征（Feature）**：用于描述数据点的属性或变量，通常而言，成千上万的特征会组成一个特征向量，用于表示一个数据点；
- **机器学习算法**：创建一个鲁棒的模型，根据若干独立的变量（特征）来预测或分类特定的输出；
- **机器学习目标**：学习一个鲁棒的映射函数，将特征从特征空间 $\mathcal{X}$ 映射到目标空间 $\mathcal{Y}$，学习一个有意义的数据映射函数是机器学习的核心问题。

### 两个阶段

- **训练阶段（Training Phase）**：使用训练数据集来调整模型参数，以最小化预测输出与预期输出之间的误差；
- **测试阶段（Testing Phase）**：使用测试数据集来评估模型的性能，确保其能够泛化到未见过的数据。

![Train and Test](./Train-and-Test.png)

### 泛化误差

存在两种形式的泛化误差：

- **欠拟合（Underfitting）**：模型过于简单，无法捕捉数据中的复杂模式，导致在训练数据和测试数据上都表现不佳；
- **过拟合（Overfitting）**：模型过于复杂，过度适应训练数据中的噪声，导致在测试数据上表现不佳。

![Underfitting and Overfitting](./Underfitting-and-Overfitting.png)

### 常见模型

支持向量机（Support Vector Machine，SVM）、决策树（Decision Tree）、k 近邻算法（k-Nearest Neighbors，k-NN）等都是常见的机器学习模型。

![Common ML Models](./Common-ML-Models.png)

而深度学习则是机器学习的一个子领域，它使用多层神经网络来建模复杂的数据模式。一个神经网络的层数就是它的“深度”。

### 深度学习

为什么需要深度？一个简单的例子是“异或（XOR）”问题——虽然单层神经网络可以实现“与（And）”、“或（Or）”这样的逻辑运算，但它们无法解决 XOR 问题。

![XOR Problem](./XOR-Problem.png)

一个单层的神经网络的决策边界是一个超平面，一个双层神经网络的决策边界是复杂多边形，一个三层神经网络的决策边界是任意曲线。

正因为单层神经网络只具有线性的决策边界，这启示我们使用多层神经网络来实现非线性决策边界，从而解决 XOR 问题。事实上，多层神经网络是一个有向加权图，它的各层之间具有连接。

![Multi-Layers](./Multi-Layers.png)

在深度学习的训练中，有两股信息流在整个系统中流通：

- **数据流**：前向传播过程中，数据逐层通过网络的运算最终汇聚到输出层；
- **误差流**：预测输出与预期输出进行比较，根据指定的误差算法计算误差，并逆向传播该误差，从而调整网络中的权重参数。
